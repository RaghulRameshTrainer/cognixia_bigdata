{"cells":[{"cell_type":"markdown","source":["d-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 400px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9807ac72-5f0b-479d-bb60-c3a9801afdb0"}}},{"cell_type":"markdown","source":["# Datetime Functions\nProcessing Datetimes\n1. Cast to timestamp\n2. Format datetimes\n3. Extract from timestamp\n4. Convert to date\n5. Manipulate datetimes\n\n##### Methods\n- Column (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=column#pyspark.sql.Column\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html\" target=\"_blank\">Scala</a>): `cast`\n- Built-In Functions (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=functions#module-pyspark.sql.functions\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_blank\">Scala</a>): `date_format`, `to_date`, `date_add`, `year`, `month`, `dayofweek`, `minute`, `second`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f53939b2-4e4e-48a7-ad1a-3db672ff84b8"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99db9d0f-d750-4995-80ea-b6a883859607"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use a subset of the BedBricks events dataset to practice working with date times."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74f1b664-de8f-47c6-8c4d-1e55312f07ae"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = spark.read.parquet(eventsPath).select(\"user_id\", col(\"event_timestamp\").alias(\"timestamp\"))\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88dac136-e2c5-4c43-9dd2-118b7529cb35"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Cast to Timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dab352a9-7ef2-409e-8238-e91d3307be58"}}},{"cell_type":"markdown","source":["#### `cast()`\nCasts column to a different data type, specified using string representation or DataType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ce1f0d9-a3ce-470c-842d-c8211d666903"}}},{"cell_type":"code","source":["timestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(\"timestamp\"))\ndisplay(timestampDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca2a74bd-4c3b-416a-9d3e-6c544545838c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\n\ntimestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(TimestampType()))\ndisplay(timestampDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95892273-91cf-4440-a171-1cb5c2e31ee3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Format date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec95c5d9-0a2f-455c-919f-7e7d496ba124"}}},{"cell_type":"markdown","source":["#### `date_format()`\nConverts a date/timestamp/string to a string formatted with the given date time pattern.\n\nSee valid date and time format patterns for <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">Spark 3</a> and <a href=\"https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\" target=\"_blank\">Spark 2</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3d59fda-e0ff-4ca5-a00b-540473e14305"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\n\nformattedDF = (timestampDF.withColumn(\"date string\", date_format(\"timestamp\", \"MMMM dd, yyyy\"))\n  .withColumn(\"time string\", date_format(\"timestamp\", \"HH:mm:ss.SSSSSS\"))\n) \ndisplay(formattedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7968461e-efdf-4e75-bee4-f99fb1266c9b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Extract datetime attribute from timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1610a249-beab-45bf-95b1-47cd046ff1aa"}}},{"cell_type":"markdown","source":["#### `year`\nExtracts the year as an integer from a given date/timestamp/string.\n\n##### Similar methods: `month`, `dayofweek`, `minute`, `second`, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce2dcb9f-1054-4d83-9b77-c1e1a6bcbfb1"}}},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, dayofweek, minute, second\n\ndatetimeDF = (timestampDF.withColumn(\"year\", year(col(\"timestamp\")))\n  .withColumn(\"month\", month(col(\"timestamp\")))\n  .withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n  .withColumn(\"minute\", minute(col(\"timestamp\")))\n  .withColumn(\"second\", second(col(\"timestamp\")))              \n)\ndisplay(datetimeDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8de64981-ce41-4097-b05d-29dfef786e68"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Convert to Date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad37237a-8152-4c8a-bf4d-fdbac34b8801"}}},{"cell_type":"markdown","source":["#### `to_date`\nConverts the column into DateType by casting rules to DateType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9463f44b-a993-4b3c-8919-651f5dcdca5d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n\ndateDF = timestampDF.withColumn(\"date\", to_date(col(\"timestamp\")))\ndisplay(dateDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be3ef84f-6dff-4837-b0cb-450faa62aa59"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Manipulate Datetimes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6e126b4-b707-4491-9749-6ff415bba195"}}},{"cell_type":"markdown","source":["#### `date_add`\nReturns the date that is the given number of days after start"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3925bac6-d22d-49a7-ab6b-7fa61436ffe3"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add\n\nplus2DF = timestampDF.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\ndisplay(plus2DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"649874a0-c22c-4b84-9e98-31c55e96773a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Active Users Lab\nPlot daily active users and average active users by day of week.\n1. Extract timestamp and date of events\n2. Get daily active users\n3. Get average number of active users by day of week\n4. Sort day of week in correct order"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d916f26-6120-443f-bf35-b1e0c3f9accd"}}},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame of user IDs and timestamps of events logged on the BedBricks website."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86f323c4-c075-487b-a439-eef75084967f"}}},{"cell_type":"code","source":["df = (spark.read.parquet(eventsPath)\n  .select(\"user_id\", col(\"event_timestamp\").alias(\"ts\")))\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd93c1d7-e598-4873-8756-9f8f4ab30c04"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Extract timestamp and date of events\n- Convert **`ts`** from microseconds to seconds by dividing by 1 million and cast to timestamp \n- Add **`date`** column by converting **`ts`** to date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"701d7d68-a163-47b7-ac48-fc61d449726c"}}},{"cell_type":"code","source":["# TODO\ndatetimeDF = (df.FILL_IN\n)\ndisplay(datetimeDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34ca2811-3526-46bc-978f-69f028663b1e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b80dfe42-fc59-4474-9873-bee334359be6"}}},{"cell_type":"code","source":["from pyspark.sql.types import DateType, StringType, StructField, StructType, TimestampType\n\nexpected1a = StructType([StructField(\"user_id\", StringType(), True),\n  StructField(\"ts\", TimestampType(), True),\n  StructField(\"date\", DateType(), True)])\n\nresult1a = datetimeDF.schema\n\nassert expected1a == result1a, \"datetimeDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a411d706-2614-4c7f-bcf9-92c7c9f1f06b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\n\nexpected1b = datetime.date(2020, 6, 19)\nresult1b = datetimeDF.sort(\"date\").first().date\n\nassert expected1b == result1b, \"datetimeDF does not have the expected date values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6b72de0-26c7-41e7-a55e-4c98c16ba652"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get daily active users\n- Group by date\n- Aggregate approximate count of distinct **`user_id`** and alias with \"active_users\"\n  - Recall built-in function to get approximate count distinct\n- Sort by date\n- Plot as line graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"613573d3-714e-4989-ae9a-b4fa5e9d7c90"}}},{"cell_type":"code","source":["# TODO\nactiveUsersDF = (datetimeDF.FILL_IN\n)\ndisplay(activeUsersDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"762f6feb-fcd8-4b5c-ae2d-b155dbd4670a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"964f1c2c-4d60-46e6-9fca-432ec7128150"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\nexpected2a = StructType([StructField(\"date\", DateType(), True),\n  StructField(\"active_users\", LongType(), False)])\n\nresult2a = activeUsersDF.schema\n\nassert expected2a == result2a, \"activeUsersDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03d25be4-d622-4dcf-979a-023991ae147e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected2b = [(datetime.date(2020, 6, 19), 251573), (datetime.date(2020, 6, 20), 357215), (datetime.date(2020, 6, 21), 305055), (datetime.date(2020, 6, 22), 239094), (datetime.date(2020, 6, 23), 243117)]\n\nresult2b = [(row.date, row.active_users) for row in activeUsersDF.take(5)]\n\nassert expected2b == result2b, \"activeUsersDF does not have the expected values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71d04fe2-9b8f-4e87-a51a-5e06f8a6d243"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Get average number of active users by day of week\n- Add **`day`** column by extracting day of week from **`date`** using a datetime pattern string\n- Group by **`day`**\n- Aggregate average of **`active_users`** and alias with \"avg_users\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8cfb53a-ce6f-4145-842f-6b8d79cbf202"}}},{"cell_type":"code","source":["# TODO\nactiveDowDF = (activeUsersDF.FILL_IN\n)\ndisplay(activeDowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f979d533-364c-4241-901b-54148fb17e19"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37f67ca5-237d-4b3a-93c2-2871efbc1a42"}}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n\nexpected3a = StructType([StructField(\"day\", StringType(), True),\n  StructField(\"avg_users\", DoubleType(), True)])\n\nresult3a = activeDowDF.schema\n\nassert expected3a == result3a, \"activeDowDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09ff46b3-fcc3-4a56-98ee-e364007185f3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected3b = [('Fri', 247180.66666666666), ('Mon', 238195.5), ('Sat', 278482.0), ('Sun', 282905.5), ('Thu', 264620.0), ('Tue', 260942.5), ('Wed', 227214.0)]\n\nresult3b = [(row.day, row.avg_users) for row in activeDowDF.sort(\"day\").collect()]\n\nassert expected3b == result3b, \"activeDowDF does not have the expected values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"befbd5f3-fae8-4b58-a845-288e50a3ceb8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2deed050-9f06-410d-b121-8f82e024611d"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67c30789-5b05-4e72-92d0-145302491b99"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.2 Datetimes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2699538775778687}},"nbformat":4,"nbformat_minor":0}
